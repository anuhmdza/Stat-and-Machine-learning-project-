{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ff6c904",
   "metadata": {},
   "source": [
    "**The Following chunk will retrieve and clean the census data, get the variables we need for the years 2016-2022 and convert the dataframe into a csv file.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4218511e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emily\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Census data saved as census_data_2016_2022.csv\n"
     ]
    }
   ],
   "source": [
    "from census import Census\n",
    "from us import states\n",
    "import pandas as pd\n",
    "\n",
    "# Replace with your Census API key\n",
    "CENSUS_API_KEY = \"fe6843e76d70b7f6bcc3ad4885d926a06f69c5c4\"\n",
    "\n",
    "c = Census(CENSUS_API_KEY)\n",
    "\n",
    "# Collect ACS data for all years\n",
    "acs_all_years = []\n",
    "\n",
    "for year in range(2016, 2023):\n",
    "    data = c.acs5.get(\n",
    "        (\"NAME\", \"B19013_001E\", \"B25064_001E\", \"B27010_001E\"),\n",
    "        geo={\"for\": \"state:*\"},\n",
    "        year=year\n",
    "    )\n",
    "    for row in data:\n",
    "        row['year'] = year\n",
    "    acs_all_years.extend(data)\n",
    "\n",
    "# Convert to DataFrame\n",
    "acs_df = pd.DataFrame(acs_all_years)\n",
    "\n",
    "# Rename columns\n",
    "acs_df.rename(columns={\n",
    "    \"NAME\": \"state_full\",\n",
    "    \"B19013_001E\": \"median_household_income\",\n",
    "    \"B25064_001E\": \"median_gross_rent\",\n",
    "    \"B27010_001E\": \"health_insurance_coverage\"\n",
    "}, inplace=True)\n",
    "\n",
    "# Add state abbreviations\n",
    "fips_to_abbr = {str(s.fips).zfill(2): s.abbr for s in states.STATES}\n",
    "acs_df['state'] = acs_df['state'].map(fips_to_abbr)\n",
    "\n",
    "acs_df = acs_df[['state', 'year', 'median_household_income', 'median_gross_rent', 'health_insurance_coverage']]\n",
    "# Save to CSV\n",
    "acs_df.to_csv(\"census_data_2016_2022.csv\", index=False)\n",
    "print(\" Census data saved as census_data_2016_2022.csv\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5a9ced",
   "metadata": {},
   "source": [
    "**This chunk retrieves and cleans NOAA data for all 50 U.S states, using only the stations that provide all the variables we will be using for the years 2016-2022.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c48d73ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emily\\AppData\\Local\\Temp\\ipykernel_18784\\325612814.py:9: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  inventory = pd.read_csv(\"ghcnd-inventory.txt\", delim_whitespace=True, names=inv_cols)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " All done! Saved: climate_monthly_2016_2022.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# Load and Filter Inventory\n",
    "\n",
    "\n",
    "inv_cols = [\"station_id\", \"latitude\", \"longitude\", \"element\", \"first_year\", \"last_year\"]\n",
    "inventory = pd.read_csv(\"ghcnd-inventory.txt\", delim_whitespace=True, names=inv_cols)\n",
    "inv_filtered = inventory[inventory[\"element\"].isin([\"TMAX\", \"TMIN\", \"PRCP\"])]\n",
    "\n",
    "# Only stations with all 3 core elements\n",
    "counts = inv_filtered.groupby(\"station_id\")[\"element\"].nunique().reset_index()\n",
    "counts = counts[counts[\"element\"] == 3]\n",
    "station_elements = inv_filtered.merge(counts[[\"station_id\"]], on=\"station_id\", how=\"inner\")\n",
    "recent_stations = station_elements[station_elements[\"last_year\"] >= 2022]\n",
    "\n",
    "# Station Metadata and Best Stations\n",
    "\n",
    "colspecs = [(0, 11), (12, 20), (21, 30), (31, 37), (38, 40), (41, 71)]\n",
    "colnames = [\"station_id\", \"lat\", \"lon\", \"elev\", \"state\", \"name\"]\n",
    "stations = pd.read_fwf(\"ghcnd-stations.txt\", colspecs=colspecs, names=colnames)\n",
    "\n",
    "merged = recent_stations.merge(stations, on=\"station_id\")\n",
    "us_states = [\n",
    "    'AL','AK','AZ','AR','CA','CO','CT','DE','FL','GA','HI','ID','IL','IN','IA','KS','KY','LA','ME','MD',\n",
    "    'MA','MI','MN','MS','MO','MT','NE','NV','NH','NJ','NM','NY','NC','ND','OH','OK','OR','PA','RI','SC',\n",
    "    'SD','TN','TX','UT','VT','VA','WA','WV','WI','WY','DC'\n",
    "]\n",
    "merged = merged[merged[\"state\"].isin(us_states)]\n",
    "best_stations = merged.sort_values(by=[\"state\", \"elev\"]).groupby(\"state\").first().reset_index()\n",
    "best_stations.to_csv(\"best_stations_per_state.csv\", index=False)\n",
    "\n",
    "# Download .dly Files\n",
    "\n",
    "os.makedirs(\"dly_files\", exist_ok=True)\n",
    "base_url = \"https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/all\"\n",
    "\n",
    "for station_id in best_stations[\"station_id\"]:\n",
    "    file_path = f\"dly_files/{station_id}.dly\"\n",
    "    if not os.path.exists(file_path):\n",
    "        url = f\"{base_url}/{station_id}.dly\"\n",
    "        r = requests.get(url)\n",
    "        if r.status_code == 200:\n",
    "            with open(file_path, \"wb\") as f:\n",
    "                f.write(r.content)\n",
    "            print(f\" Downloaded {station_id}.dly\")\n",
    "        else:\n",
    "            print(f\"Failed to download {station_id}\")\n",
    "\n",
    "# Parse .dly Files for 2016â€“2022\n",
    "\n",
    "\n",
    "def parse_dly_file(file_path, station_id, state, years=range(2016, 2023)):\n",
    "    records = []\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            year = int(line[11:15])\n",
    "            month = int(line[15:17])\n",
    "            element = line[17:21]\n",
    "\n",
    "            if year not in years or element not in [\"TMAX\", \"TMIN\", \"PRCP\"]:\n",
    "                continue\n",
    "\n",
    "            for day in range(1, 32):\n",
    "                value_str = line[21 + (day - 1) * 8 : 26 + (day - 1) * 8]\n",
    "                try:\n",
    "                    value = int(value_str[:5])\n",
    "                    if value == -9999:\n",
    "                        continue\n",
    "                    date = pd.to_datetime(f\"{year}-{month:02d}-{day:02d}\", errors=\"coerce\")\n",
    "                    if pd.notna(date):\n",
    "                        records.append({\n",
    "                            \"date\": date,\n",
    "                            \"state\": state,\n",
    "                            \"element\": element,\n",
    "                            \"value\": value / 10\n",
    "                        })\n",
    "                except:\n",
    "                    continue\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# Parse all files\n",
    "all_dfs = []\n",
    "for _, row in best_stations.iterrows():\n",
    "    path = f\"dly_files/{row['station_id']}.dly\"\n",
    "    if os.path.exists(path):\n",
    "        df = parse_dly_file(path, row[\"station_id\"], row[\"state\"])\n",
    "        all_dfs.append(df)\n",
    "\n",
    "df_all = pd.concat(all_dfs)\n",
    "\n",
    "#Pivot + Monthly Aggregation\n",
    "\n",
    "df_all[\"year\"] = df_all[\"date\"].dt.year\n",
    "df_all[\"month\"] = df_all[\"date\"].dt.month\n",
    "\n",
    "monthly = df_all.groupby([\"state\", \"year\", \"month\", \"element\"])[\"value\"].mean().unstack().reset_index()\n",
    "monthly.columns.name = None\n",
    "monthly.rename(columns={\"PRCP\": \"Precip_mm\", \"TMAX\": \"Max_Temp_C\", \"TMIN\": \"Min_Temp_C\"}, inplace=True)\n",
    "\n",
    "monthly.to_csv(\"climate_monthly_2016_2022.csv\", index=False)\n",
    "print(\" All done! Saved: climate_monthly_2016_2022.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef944ace",
   "metadata": {},
   "source": [
    "**In the following two chunks we will clean and retrieve the information from the NNDSS Annual Summary Data for Diphtheria, Tetanus, Pertussis, Measles, Mumps, Rubella for the years 2016-2022**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27dda856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Notes', 'Disease', 'Disease Code', 'Year', 'Year Code', 'Regions/States', 'Regions/States Code', 'Case Count']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Notes</th>\n",
       "      <th>Disease</th>\n",
       "      <th>Disease Code</th>\n",
       "      <th>Year</th>\n",
       "      <th>Year Code</th>\n",
       "      <th>Regions/States</th>\n",
       "      <th>Regions/States Code</th>\n",
       "      <th>Case Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Diphtheria</td>\n",
       "      <td>10040.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>United States</td>\n",
       "      <td>U.S.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Diphtheria</td>\n",
       "      <td>10040.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>New England</td>\n",
       "      <td>REGION1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Diphtheria</td>\n",
       "      <td>10040.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>Connecticut</td>\n",
       "      <td>09</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Diphtheria</td>\n",
       "      <td>10040.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>Maine</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Diphtheria</td>\n",
       "      <td>10040.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>categorized as Suppressed. More information: h...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>7. Values in charts and maps refer to statisti...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>designation assigned to category labels, such ...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>8. Any variation of disease incidence by race ...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>cultural, behavioral, and social factors inclu...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>598 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Notes     Disease  \\\n",
       "0                                                  NaN  Diphtheria   \n",
       "1                                                  NaN  Diphtheria   \n",
       "2                                                  NaN  Diphtheria   \n",
       "3                                                  NaN  Diphtheria   \n",
       "4                                                  NaN  Diphtheria   \n",
       "..                                                 ...         ...   \n",
       "593  categorized as Suppressed. More information: h...        None   \n",
       "594  7. Values in charts and maps refer to statisti...        None   \n",
       "595  designation assigned to category labels, such ...        None   \n",
       "596  8. Any variation of disease incidence by race ...        None   \n",
       "597  cultural, behavioral, and social factors inclu...        None   \n",
       "\n",
       "     Disease Code    Year  Year Code Regions/States Regions/States Code  \\\n",
       "0         10040.0  2022.0     2022.0  United States                U.S.   \n",
       "1         10040.0  2022.0     2022.0    New England             REGION1   \n",
       "2         10040.0  2022.0     2022.0    Connecticut                  09   \n",
       "3         10040.0  2022.0     2022.0          Maine                  23   \n",
       "4         10040.0  2022.0     2022.0  Massachusetts                  25   \n",
       "..            ...     ...        ...            ...                 ...   \n",
       "593           NaN     NaN        NaN           None                None   \n",
       "594           NaN     NaN        NaN           None                None   \n",
       "595           NaN     NaN        NaN           None                None   \n",
       "596           NaN     NaN        NaN           None                None   \n",
       "597           NaN     NaN        NaN           None                None   \n",
       "\n",
       "    Case Count  \n",
       "0            1  \n",
       "1            0  \n",
       "2            0  \n",
       "3            0  \n",
       "4            0  \n",
       "..         ...  \n",
       "593       None  \n",
       "594       None  \n",
       "595       None  \n",
       "596       None  \n",
       "597       None  \n",
       "\n",
       "[598 rows x 8 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Try reading it as a tab-separated file instead of Excel\n",
    "nndss_raw = pd.read_csv(\"NNDSS Annual Summary Data 2016-2022 (2).xls\", sep=\"\\t\", engine=\"python\", encoding=\"utf-8\")\n",
    "\n",
    "# Display column names\n",
    "print(nndss_raw.columns.tolist())\n",
    "\n",
    "# Quick look\n",
    "nndss_raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ae6a215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cleaned and saved as 'nndss_dtp_mmr_cleaned.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the file (tab-delimited)\n",
    "nndss_df = pd.read_csv(\"NNDSS Annual Summary Data 2016-2022 (2).xls\", sep=\"\\t\", engine=\"python\")\n",
    "\n",
    "# Keep only the columns we need\n",
    "nndss_df = nndss_df[[\"Year\", \"Regions/States\", \"Disease\", \"Case Count\"]]\n",
    "\n",
    "# Rename for clarity\n",
    "nndss_df.columns = [\"year\", \"state\", \"disease\", \"cases\"]\n",
    "\n",
    "# Filter to diseases of interest\n",
    "diseases_of_interest = [\"Diphtheria\", \"Tetanus\", \"Pertussis\", \"Measles\", \"Mumps\", \"Rubella\"]\n",
    "nndss_df = nndss_df[nndss_df[\"disease\"].isin(diseases_of_interest)]\n",
    "\n",
    "# Remove rows where state is US Territories or Totals\n",
    "us_states = [\n",
    "    'Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California', 'Colorado', 'Connecticut', 'Delaware',\n",
    "    'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana', 'Iowa', 'Kansas', 'Kentucky',\n",
    "    'Louisiana', 'Maine', 'Maryland', 'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi',\n",
    "    'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire', 'New Jersey', 'New Mexico',\n",
    "    'New York', 'North Carolina', 'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania',\n",
    "    'Rhode Island', 'South Carolina', 'South Dakota', 'Tennessee', 'Texas', 'Utah', 'Vermont',\n",
    "    'Virginia', 'Washington', 'West Virginia', 'Wisconsin', 'Wyoming'\n",
    "]\n",
    "nndss_df = nndss_df[nndss_df[\"state\"].isin(us_states)]\n",
    "\n",
    "# Map state names to abbreviations\n",
    "import us\n",
    "state_abbrev = {state.name: state.abbr for state in us.states.STATES}\n",
    "nndss_df[\"state\"] = nndss_df[\"state\"].map(state_abbrev)\n",
    "\n",
    "# Convert case count to numeric\n",
    "nndss_df[\"cases\"] = pd.to_numeric(nndss_df[\"cases\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "# Final clean dataframe\n",
    "nndss_cleaned = nndss_df[[\"state\", \"year\", \"disease\", \"cases\"]]\n",
    "\n",
    "# Save it\n",
    "nndss_cleaned.to_csv(\"nndss_dtp_mmr_cleaned.csv\", index=False)\n",
    "print(\" Cleaned and saved as 'nndss_dtp_mmr_cleaned.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fba2be",
   "metadata": {},
   "source": [
    "**This chunk retrieves and cleans the vaccination coverage and exemption among kindergartners for the years 2016-2022**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3be91c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Vaccination data cleaned and saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "vacc = pd.read_csv(\"Vaccination_Coverage_and_Exemptions_among_Kindergartners_20250326.csv\")\n",
    "\n",
    "# Filter to only 'States' geography\n",
    "vacc = vacc[vacc[\"Geography Type\"] == \"States\"]\n",
    "\n",
    "# Extract year from 'School Year' like \"2021-22\"\n",
    "vacc[\"year\"] = vacc[\"School Year\"].str.extract(r\"(\\d{4})\").astype(float)\n",
    "\n",
    "# Filter to 2016â€“2022\n",
    "vacc = vacc[vacc[\"year\"].between(2016, 2022)]\n",
    "\n",
    "# Clean and keep relevant columns\n",
    "vacc = vacc[[\"Geography\", \"year\", \"Vaccine/Exemption\", \"Estimate (%)\"]]\n",
    "vacc.rename(columns={\"Geography\": \"state\"}, inplace=True)\n",
    "\n",
    "# Convert Estimate to numeric (force errors to NaN)\n",
    "vacc[\"Estimate (%)\"] = pd.to_numeric(vacc[\"Estimate (%)\"], errors=\"coerce\")\n",
    "\n",
    "# Pivot the table: One column per vaccine or exemption type\n",
    "vacc_pivot = vacc.pivot_table(\n",
    "    index=[\"state\", \"year\"],\n",
    "    columns=\"Vaccine/Exemption\",\n",
    "    values=\"Estimate (%)\",\n",
    "    aggfunc=\"mean\"  # Handle duplicates\n",
    ").reset_index()\n",
    "\n",
    "# Save result\n",
    "vacc_pivot.to_csv(\"cleaned_vaccination_data_2016_2022.csv\", index=False)\n",
    "print(\" Vaccination data cleaned and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640228f5",
   "metadata": {},
   "source": [
    "**The follwoing chunks will merge all our previous datasets to analyze,visualize and model ready**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a3f49d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Final dataset saved as 'final_model_dataset_2016_2022.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "\n",
    "vacc = pd.read_csv(\"cleaned_vaccination_data_2016_2022.csv\")\n",
    "nndss = pd.read_csv(\"nndss_dtp_mmr_cleaned.csv\")\n",
    "climate = pd.read_csv(\"climate_monthly_2016_2022.csv\")\n",
    "census = pd.read_csv(\"census_data_2016_2022.csv\")\n",
    "\n",
    "# Preprocess vaccination data\n",
    "\n",
    "# Ensure correct types\n",
    "vacc['year'] = vacc['year'].astype(int)\n",
    "\n",
    "# Map state names to abbreviations\n",
    "state_mapping = {\n",
    "    'Alabama': 'AL', 'Alaska': 'AK', 'Arizona': 'AZ', 'Arkansas': 'AR', 'California': 'CA',\n",
    "    'Colorado': 'CO', 'Connecticut': 'CT', 'Delaware': 'DE', 'Florida': 'FL', 'Georgia': 'GA',\n",
    "    'Hawaii': 'HI', 'Idaho': 'ID', 'Illinois': 'IL', 'Indiana': 'IN', 'Iowa': 'IA',\n",
    "    'Kansas': 'KS', 'Kentucky': 'KY', 'Louisiana': 'LA', 'Maine': 'ME', 'Maryland': 'MD',\n",
    "    'Massachusetts': 'MA', 'Michigan': 'MI', 'Minnesota': 'MN', 'Mississippi': 'MS', 'Missouri': 'MO',\n",
    "    'Montana': 'MT', 'Nebraska': 'NE', 'Nevada': 'NV', 'New Hampshire': 'NH', 'New Jersey': 'NJ',\n",
    "    'New Mexico': 'NM', 'New York': 'NY', 'North Carolina': 'NC', 'North Dakota': 'ND', 'Ohio': 'OH',\n",
    "    'Oklahoma': 'OK', 'Oregon': 'OR', 'Pennsylvania': 'PA', 'Rhode Island': 'RI', 'South Carolina': 'SC',\n",
    "    'South Dakota': 'SD', 'Tennessee': 'TN', 'Texas': 'TX', 'Utah': 'UT', 'Vermont': 'VT',\n",
    "    'Virginia': 'VA', 'Washington': 'WA', 'West Virginia': 'WV', 'Wisconsin': 'WI', 'Wyoming': 'WY'\n",
    "}\n",
    "vacc['state'] = vacc['state'].map(state_mapping)\n",
    "\n",
    "# Preprocess NNDSS outbreak data\n",
    "\n",
    "nndss['year'] = nndss['year'].astype(int)\n",
    "nndss['state'] = nndss['state'].str.upper()\n",
    "\n",
    "# Aggregate by state and year\n",
    "nndss_grouped = (\n",
    "    nndss.groupby(['state', 'year'])['cases']\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "#Preprocess climate data\n",
    "\n",
    "climate['year'] = climate['year'].astype(int)\n",
    "climate['state'] = climate['state'].str.upper()\n",
    "\n",
    "# Aggregate monthly to yearly per state\n",
    "climate_grouped = (\n",
    "    climate.groupby(['state', 'year']).agg({\n",
    "        'Precip_mm': 'mean',\n",
    "        'Max_Temp_C': 'mean',\n",
    "        'Min_Temp_C': 'mean'\n",
    "    }).reset_index()\n",
    ")\n",
    "\n",
    "#Preprocess census data\n",
    "\n",
    "\n",
    "census['year'] = census['year'].astype(int)\n",
    "census['state'] = census['state'].str.upper()\n",
    "\n",
    "# Merge all data\n",
    "\n",
    "merged = climate_grouped.merge(census, on=['state', 'year'], how='left')\n",
    "merged = merged.merge(vacc, on=['state', 'year'], how='left')\n",
    "merged = merged.merge(nndss_grouped, on=['state', 'year'], how='left')\n",
    "\n",
    "# Fill missing cases with 0\n",
    "merged['cases'] = merged['cases'].fillna(0).astype(int)\n",
    "\n",
    "#Export final dataset\n",
    "\n",
    "\n",
    "merged.to_csv(\"final_model_dataset_2016_2022.csv\", index=False)\n",
    "print(\" Final dataset saved as 'final_model_dataset_2016_2022.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8bed36a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "state                         0\n",
       "year                          0\n",
       "Precip_mm                     0\n",
       "Max_Temp_C                    6\n",
       "Min_Temp_C                    6\n",
       "median_household_income       0\n",
       "median_gross_rent             0\n",
       "health_insurance_coverage     0\n",
       "DTP, DTaP, or DT              7\n",
       "Exemption                     4\n",
       "Hepatitis B                  25\n",
       "MMR                           6\n",
       "MMR (PAC)                    49\n",
       "Polio                         6\n",
       "Varicella                     6\n",
       "cases                         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged\n",
    "merged.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82cbec64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "state                         0.000000\n",
       "year                          0.000000\n",
       "Precip_mm                     0.000000\n",
       "Max_Temp_C                    1.929260\n",
       "Min_Temp_C                    1.929260\n",
       "median_household_income       0.000000\n",
       "median_gross_rent             0.000000\n",
       "health_insurance_coverage     0.000000\n",
       "DTP, DTaP, or DT              2.250804\n",
       "Exemption                     1.286174\n",
       "Hepatitis B                   8.038585\n",
       "MMR                           1.929260\n",
       "MMR (PAC)                    15.755627\n",
       "Polio                         1.929260\n",
       "Varicella                     1.929260\n",
       "cases                         0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent= merged.isnull().sum() * 100/len(merged)\n",
    "percent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1345ff9e",
   "metadata": {},
   "source": [
    "**This creates a new merged dataset for models that dont handle missing values well. We imputed the missing values with median per state and after that we still had missing values, so we will do overall median. The missing values were due to lack of reporting from those states**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd92b8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Final dataset imputed by state and globally. Saved as 'final_model_dataset_2016_2022_imputed_by_state.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emily\\AppData\\Local\\Temp\\ipykernel_18784\\3495534489.py:31: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "C:\\Users\\emily\\AppData\\Local\\Temp\\ipykernel_18784\\3495534489.py:31: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"final_model_dataset_2016_2022.csv\")\n",
    "\n",
    "# Columns to impute\n",
    "vaccine_cols = [\n",
    "    'DTP, DTaP, or DT', 'MMR', 'MMR (PAC)', 'Polio', 'Varicella',\n",
    "    'Hepatitis B', 'Exemption'\n",
    "]\n",
    "numeric_cols = [\n",
    "    'Precip_mm', 'Max_Temp_C', 'Min_Temp_C',\n",
    "    'median_household_income', 'median_gross_rent', 'health_insurance_coverage'\n",
    "]\n",
    "\n",
    "cols_to_fill = vaccine_cols + numeric_cols\n",
    "\n",
    "# Create missing flags for ALL columns to be imputed\n",
    "\n",
    "for col in cols_to_fill:\n",
    "    df[f\"{col}_missing\"] = df[col].isna().astype(int)\n",
    "\n",
    "# Impute by median per state\n",
    "\n",
    "df[cols_to_fill] = df.groupby(\"state\")[cols_to_fill].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "# Fill any remaining NaNs with overall median\n",
    "\n",
    "for col in cols_to_fill:\n",
    "    if df[col].isna().sum() > 0:\n",
    "        df[col].fillna(df[col].median(), inplace=True)\n",
    "        \n",
    "        \n",
    "#Save the result\n",
    "\n",
    "df.to_csv(\"final_model_dataset_2016_2022_imputed_by_state.csv\", index=False)\n",
    "print(\" Final dataset imputed by state and globally. Saved as 'final_model_dataset_2016_2022_imputed_by_state.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2edef1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Missing Values:\n",
      " Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "missing = df.isnull().sum()\n",
    "print(\"\\n Missing Values:\\n\", missing[missing > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "68f8c83a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "state                                0.0\n",
       "year                                 0.0\n",
       "Precip_mm                            0.0\n",
       "Max_Temp_C                           0.0\n",
       "Min_Temp_C                           0.0\n",
       "median_household_income              0.0\n",
       "median_gross_rent                    0.0\n",
       "health_insurance_coverage            0.0\n",
       "DTP, DTaP, or DT                     0.0\n",
       "Exemption                            0.0\n",
       "Hepatitis B                          0.0\n",
       "MMR                                  0.0\n",
       "MMR (PAC)                            0.0\n",
       "Polio                                0.0\n",
       "Varicella                            0.0\n",
       "cases                                0.0\n",
       "DTP, DTaP, or DT_missing             0.0\n",
       "MMR_missing                          0.0\n",
       "MMR (PAC)_missing                    0.0\n",
       "Polio_missing                        0.0\n",
       "Varicella_missing                    0.0\n",
       "Hepatitis B_missing                  0.0\n",
       "Exemption_missing                    0.0\n",
       "Precip_mm_missing                    0.0\n",
       "Max_Temp_C_missing                   0.0\n",
       "Min_Temp_C_missing                   0.0\n",
       "median_household_income_missing      0.0\n",
       "median_gross_rent_missing            0.0\n",
       "health_insurance_coverage_missing    0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent= df.isnull().sum() * 100/len(df)\n",
    "percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1741715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fcea61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96814b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
